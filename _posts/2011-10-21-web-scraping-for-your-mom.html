--- 
layout: post
title: Web Scraping for your Mom
tags: 
- Coding
- crawling
- scraping
- web
status: publish
type: post
published: true
meta: 
  _edit_last: "27826612"
  jabber_published: "1319157757"
  email_notification: "1319157760"
---
<a href="http://pennyhacks.files.wordpress.com/2011/10/data_scraping1.jpg"><img style="display:block;margin:auto;" class="alignnone size-full wp-image-69" title="data_scraping" src="http://pennyhacks.files.wordpress.com/2011/10/data_scraping1.jpg" alt="" width="400" height="308" /></a>

<strong>in a nutshell:</strong> This post will be a little different in nature as I'm still working on a couple hacks that will require a few more weeks of work. In the interim I'll explain the very basics of web scraping/crawling. This is a great skill to have as it will open the doors to the limitless amounts of data from the web.

<strong>what you need:</strong>
<ol>
	<li>Python</li>
	<li>HTML Parser (<a title="PyQuery" href="http://pypi.python.org/pypi/pyquery" target="_blank">PyQuery</a>, <a title="BeautifulSoup" href="http://www.crummy.com/software/BeautifulSoup/" target="_blank">BeautifulSoup</a>, <a href="http://stackoverflow.com/questions/717541/parsing-html-in-python" target="_blank">etc</a>) Note that this is optional, you can do the parsing yourself with regexps</li>
</ol>
<div><strong>implementation:</strong> To start we'll go over the very basic idea. The goal is to have a few starter links, and either scrape more links off of them or scrape the data and store it into a database. To do this we'll use the Python module <code>urllib2</code>. Here's is a sample starter script:</div>
<pre><code> 
#!/usr/bin/env python<br />
import sys<br /> 
import os <br />
import urllib2 #Python module for making web requests<br />
import re #Python module for using regular expressions<br /> 
</code></pre>
Easy so far? Now you're all set up to start the fun stuff. For sake of example, we'll be scraping Hacker News:
<pre><code> 
BASE_SITE = 'http://news.ycombinator.com/'<br /> 
TIMEOUT = 15 request = urllib2.Request(BASE_SITE) #Create a request object<br />
page = None<br /> 
try:<br />
    handler = urllib2.urlopen(request, timeout=TIMEOUT) #Make the http request<br />
    page = handler.read() #Get the huge glob of html<br />
    handler.close() <br />
except IOError: <br />
    print 'Failure to open site' #Handle failure <br />
    sys.exit(0)<br /> </code></pre>
To start, the Python request object can specify a ton more stuff and it is also how you would spoof your header. You can read about it <a href="http://docs.python.org/library/urllib2.html#urllib2.Request">here</a>. Then we make the http request which will timeout after TIMEOUT seconds. If we're successful in opening the page we can read all the html by calling read(). There are also other functions supplied that allow you to get the website status code and other fun things. Now that we have this blob, (this is where it is nice to have a html parser) we'll use a regex to extract the data we want.
<pre><code> 
linkPattern = r'href="(.*?)"' #Regex for grabbing all the links on the page <br />
linkRe = re.compile(linkPattern) <br />
matchLinks = linkRe.findall(page) #Finding all the links on the page <br />
print matchLinks </code></pre>
Bam. There you go, you now have a whole list of links in which the possibilities are endless. You can repeat the same process on each of these links to scrape their data and so on. This is a super basic example of web scraping but it can help you get off the ground.

<strong>areas to explore: </strong>Now that you know the basics you can do so much more
<ul>
	<li>Set up a <a href="http://unixgeeks.org/security/newbie/unix/cron-1.html">cron job</a> to run your scraper monthly, daily, or even every second (easily save every article on Hacker News)</li>
	<li>Try using an html parser to pull more information from a page</li>
	<li>Gather data to use a <a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">machine learning</a> algorithm on</li>
</ul>
Get the script <a href="https://github.com/rudolphben/sampleWebScraper">here</a>
